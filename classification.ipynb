{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f83c01-9e63-408a-aee5-d5b0e7f634fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ilukerogers/Desktop/computerScience/MATH354/project/postings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ilukerogers/Desktop/computerScience/MATH354/project/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load all datasets\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m postings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     21\u001b[0m benefits \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenefits.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     22\u001b[0m job_industries \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_industries.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ilukerogers/Desktop/computerScience/MATH354/project/postings.csv'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Base path for project\n",
    "base_path = r\"/Users/ilukerogers/Desktop/computerScience/MATH354/project/\"\n",
    "\n",
    "# Load all datasets\n",
    "postings = pd.read_csv(os.path.join(base_path, \"postings.csv\"))\n",
    "benefits = pd.read_csv(os.path.join(base_path, \"benefits.csv\"))\n",
    "job_industries = pd.read_csv(os.path.join(base_path, \"job_industries.csv\"))\n",
    "job_skills = pd.read_csv(os.path.join(base_path, \"job_skills.csv\"))\n",
    "companies = pd.read_csv(os.path.join(base_path, \"companies.csv\"))\n",
    "employee_counts = pd.read_csv(os.path.join(base_path, \"employee_counts.csv\"))\n",
    "company_industries = pd.read_csv(os.path.join(base_path, \"company_industries.csv\"))\n",
    "company_specialities = pd.read_csv(os.path.join(base_path, \"company_specialities.csv\"))\n",
    "industries = pd.read_csv(os.path.join(base_path, \"industries.csv\"))\n",
    "salaries = pd.read_csv(os.path.join(base_path, \"salaries.csv\"))\n",
    "skills = pd.read_csv(os.path.join(base_path, \"skills.csv\"))\n",
    "\n",
    "# Check the number of unique values in columns for each dataset\n",
    "unique_values = {\n",
    "    'postings': postings['job_id'].nunique(),\n",
    "    'benefits': benefits['job_id'].nunique(),\n",
    "    'job_industries': job_industries['job_id'].nunique(),\n",
    "    'job_skills': job_skills['job_id'].nunique(),\n",
    "    'companies': companies['company_id'].nunique(),\n",
    "    'employee_counts': employee_counts['company_id'].nunique(),\n",
    "    'company_industries': company_industries['company_id'].nunique(),\n",
    "    'company_specialities': company_specialities['company_id'].nunique()\n",
    "}\n",
    "\n",
    "unique_values\n",
    "print(unique_values)\n",
    "\n",
    "# Merge job related datasets with one to one relationships\n",
    "merged_jobs = pd.merge(postings, benefits, on='job_id', how='left')\n",
    "\n",
    "# Merge company related datasets with one to one relationships\n",
    "merged_companies = pd.merge(companies, employee_counts, on='company_id', how='left')\n",
    "\n",
    "# Merge\n",
    "comprehensive_data_one_to_one = pd.merge(merged_jobs, merged_companies, on='company_id', how='left')\n",
    "print(comprehensive_data_one_to_one.head())\n",
    "\n",
    "# Identify missing data\n",
    "missing_data = comprehensive_data_one_to_one.isnull().sum()\n",
    "\n",
    "# Display columns with missing data\n",
    "significant_missing_columns = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "print(significant_missing_columns)\n",
    "\n",
    "# Handling missing values\n",
    "\n",
    "# Use \"Not Specified\" for categorical columns with missing values\n",
    "cols_fill_not_specified = [\n",
    "    'skills_desc', 'type', 'pay_period', 'currency', 'compensation_type', 'posting_domain',\n",
    "    'application_url', 'formatted_experience_level', 'company_size', 'zip_code', 'address',\n",
    "    'state', 'url', 'city', 'country', 'name'\n",
    "]\n",
    "for col in cols_fill_not_specified:\n",
    "    if col in comprehensive_data_one_to_one.columns:\n",
    "        comprehensive_data_one_to_one[col].fillna(\"Not Specified\", inplace=True)\n",
    "\n",
    "# Fill numerical columns with zeros\n",
    "cols_fill_zero = ['applies', 'views', 'follower_count', 'employee_count']\n",
    "for col in cols_fill_zero:\n",
    "    if col in comprehensive_data_one_to_one.columns:\n",
    "        comprehensive_data_one_to_one[col].fillna(0, inplace=True)\n",
    "\n",
    "# Fill 'remote_allowed' with \"Unknown\"\n",
    "if 'remote_allowed' in comprehensive_data_one_to_one.columns:\n",
    "    comprehensive_data_one_to_one['remote_allowed'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# For salary and other complex columns, leave NaNs for specific handling later\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing = comprehensive_data_one_to_one.isnull().sum()\n",
    "remaining_missing_cols = remaining_missing[remaining_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(remaining_missing_cols)\n",
    "\n",
    "# Handle even more missing values\n",
    "\n",
    "# Use \"Still Open\" for closed_time\n",
    "if 'closed_time' in comprehensive_data_one_to_one.columns:\n",
    "    comprehensive_data_one_to_one['closed_time'].fillna(\"Still Open\", inplace=True)\n",
    "\n",
    "# Use \"Unknown\" for inferred\n",
    "if 'inferred' in comprehensive_data_one_to_one.columns:\n",
    "    comprehensive_data_one_to_one['inferred'].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Use \"Not Specified\" for company description\n",
    "if 'description_y' in comprehensive_data_one_to_one.columns:\n",
    "    comprehensive_data_one_to_one['description_y'].fillna(\"Not Specified\", inplace=True)\n",
    "\n",
    "# Check for remaining missing values\n",
    "remaining_missing = comprehensive_data_one_to_one.isnull().sum()\n",
    "remaining_missing_cols = remaining_missing[remaining_missing > 0].sort_values(ascending=False)\n",
    "\n",
    "print(remaining_missing_cols)\n",
    "\n",
    "# Fill missing job descriptions\n",
    "comprehensive_data_one_to_one['description_x'].fillna(\"Not Specified\", inplace=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "comprehensive_data_cleaned = comprehensive_data_one_to_one.drop_duplicates()\n",
    "\n",
    "# Shape of the cleaned data\n",
    "print(comprehensive_data_cleaned.shape)\n",
    "\n",
    "# Most in-demand skills across job postings using the correct column\n",
    "top_skills = job_skills['skill_abr'].value_counts().head(10)\n",
    "\n",
    "# Most in-demand skills across job postings using the correct column\n",
    "top_skills = job_skills['skill_abr'].value_counts().head(10)\n",
    "\n",
    "# Set Matplotlib backend to 'TkAgg' for proper plot rendering in PyCharm\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# Most in-demand skills across job postings using the correct column\n",
    "top_skills = job_skills['skill_abr'].value_counts().head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "top_skills.plot(kind='bar', color='lightcoral')\n",
    "plt.title('Top 10 Most In-Demand Skills Across Job Postings')\n",
    "plt.xlabel('Skill Abbreviation')\n",
    "plt.ylabel('Number of Mentions in Job Postings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"breakpoint\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load the pre-processed job market data\n",
    "# Assuming comprehensive_data_cleaned is already available from your data processing\n",
    "\n",
    "# Enhanced Feature Engineering with more detailed output\n",
    "def prepare_stock_features(job_data, skills_data, company_industries_data):\n",
    "    \"\"\"\n",
    "    Prepare features for stock prediction with detailed progress reporting\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Feature Engineering ===\")\n",
    "\n",
    "    # 1. Company-level features\n",
    "    print(\"Calculating company-level features...\")\n",
    "    company_features = job_data.groupby('company_id').agg({\n",
    "        'applies': ['sum', 'mean', 'std'],\n",
    "        'views': ['sum', 'mean', 'std'],\n",
    "        'remote_allowed': lambda x: (x == 't').mean(),\n",
    "        'employee_count': ['mean', 'max'],\n",
    "        'follower_count': ['mean', 'max']\n",
    "    })\n",
    "    company_features.columns = ['_'.join(col).strip() for col in company_features.columns.values]\n",
    "    print(f\"Created {len(company_features.columns)} company-level features\")\n",
    "\n",
    "    # 2. Add industry information\n",
    "    print(\"\\nAdding industry information...\")\n",
    "    if 'industry' in company_industries_data.columns:\n",
    "        company_industries = company_industries_data.groupby('company_id')['industry'].nunique().reset_index()\n",
    "        company_industries.columns = ['company_id', 'industry_count']\n",
    "        company_features = company_features.merge(company_industries, on='company_id', how='left')\n",
    "        print(\"Added industry count features\")\n",
    "    else:\n",
    "        print(\"Warning: Industry data not available - skipping industry features\")\n",
    "\n",
    "    # 3. Add skills demand\n",
    "    print(\"\\nAdding skills information...\")\n",
    "    company_skills = skills_data.groupby('company_id')['skill_abr'].nunique().reset_index()\n",
    "    company_skills.columns = ['company_id', 'unique_skills_count']\n",
    "    company_features = company_features.merge(company_skills, on='company_id', how='left')\n",
    "    print(\"Added skills count features\")\n",
    "\n",
    "    # Fill any remaining NA values\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    initial_missing = company_features.isna().sum().sum()\n",
    "    company_features.fillna(0, inplace=True)\n",
    "    final_missing = company_features.isna().sum().sum()\n",
    "    print(f\"Filled {initial_missing} missing values (remaining: {final_missing})\")\n",
    "\n",
    "    print(\"\\n=== Feature Engineering Complete ===\")\n",
    "    print(f\"Final feature matrix shape: {company_features.shape}\")\n",
    "    print(\"Feature columns:\", list(company_features.columns))\n",
    "\n",
    "    return company_features\n",
    "\n",
    "\n",
    "# Enhanced stock data fetching with more detailed output\n",
    "def get_stock_data(company_ids, companies_df, lookahead_days=30):\n",
    "    \"\"\"\n",
    "    Fetch stock data with detailed progress reporting\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Fetching Stock Data ===\")\n",
    "\n",
    "    # Sample ticker mapping (replace with your actual mapping)\n",
    "    sample_tickers = {\n",
    "        'amazon': 'AMZN',\n",
    "        'microsoft': 'MSFT',\n",
    "        'google': 'GOOGL',\n",
    "        'apple': 'AAPL',\n",
    "        'facebook': 'META',\n",
    "        'tesla': 'TSLA',\n",
    "        'netflix': 'NFLX',\n",
    "        'oracle': 'ORCL',\n",
    "        'ibm': 'IBM',\n",
    "        'intel': 'INTC'\n",
    "    }\n",
    "\n",
    "    stock_data = []\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    print(f\"Attempting to fetch data for {len(company_ids)} companies...\")\n",
    "\n",
    "    for i, company_id in enumerate(company_ids[:20]):  # Limit to first 20 for demo\n",
    "        # Get company name\n",
    "        company_info = companies_df[companies_df['company_id'] == company_id]\n",
    "        if len(company_info) == 0:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        company_name = company_info['name'].values[0]\n",
    "\n",
    "        # Find matching ticker\n",
    "        ticker = None\n",
    "        for name_part in str(company_name).lower().split():\n",
    "            if name_part in sample_tickers:\n",
    "                ticker = sample_tickers[name_part]\n",
    "                break\n",
    "\n",
    "        if not ticker:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Get historical data\n",
    "            print(f\"\\nFetching data for {company_name} ({ticker})...\")\n",
    "            stock = yf.Ticker(ticker)\n",
    "            hist = stock.history(period=\"1y\")\n",
    "\n",
    "            if len(hist) < 30:\n",
    "                print(f\"Insufficient data for {ticker} - skipping\")\n",
    "                fail_count += 1\n",
    "                continue\n",
    "\n",
    "            # Calculate future returns\n",
    "            hist['future_close'] = hist['Close'].shift(-lookahead_days)\n",
    "            hist['future_return'] = (hist['future_close'] - hist['Close']) / hist['Close']\n",
    "\n",
    "            # Add company info\n",
    "            hist['company_id'] = company_id\n",
    "            hist['company_name'] = company_name\n",
    "            hist['ticker'] = ticker\n",
    "\n",
    "            # Keep recent data\n",
    "            recent_data = hist.iloc[-60:-lookahead_days]\n",
    "            stock_data.append(recent_data)\n",
    "            success_count += 1\n",
    "\n",
    "            print(f\"Successfully retrieved {len(recent_data)} days of data\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get data for {company_name}: {str(e)}\")\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "    print(\"\\n=== Stock Data Fetching Complete ===\")\n",
    "    print(f\"Successfully retrieved data for {success_count} companies\")\n",
    "    print(f\"Failed to retrieve data for {fail_count} companies\")\n",
    "\n",
    "    if not stock_data:\n",
    "        print(\"\\nWarning: No stock data retrieved. Using sample data for demonstration.\")\n",
    "        sample_company = companies_df.iloc[0]['company_id']\n",
    "        sample_name = companies_df.iloc[0]['name']\n",
    "        dates = pd.date_range(end=datetime.today(), periods=60)\n",
    "        sample_df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'Close': np.random.normal(100, 10, 60),\n",
    "            'company_id': sample_company,\n",
    "            'company_name': sample_name,\n",
    "            'ticker': 'SAMPLE'\n",
    "        })\n",
    "        sample_df['future_close'] = sample_df['Close'].shift(-30)\n",
    "        sample_df['future_return'] = (sample_df['future_close'] - sample_df['Close']) / sample_df['Close']\n",
    "        sample_df.set_index('Date', inplace=True)\n",
    "        stock_data.append(sample_df.dropna())\n",
    "\n",
    "    return pd.concat(stock_data)\n",
    "\n",
    "\n",
    "# Enhanced Model Training with detailed output\n",
    "def train_and_evaluate_model(X, y):\n",
    "    \"\"\"\n",
    "    Train and evaluate model with detailed reporting\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Model Training ===\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Train set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "    # Build model pipeline\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\n=== Model Evaluation ===\")\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Train MSE': mean_squared_error(y_train, y_pred_train),\n",
    "        'Test MSE': mean_squared_error(y_test, y_pred_test),\n",
    "        'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
    "        'Train R2': r2_score(y_train, y_pred_train),\n",
    "        'Test R2': r2_score(y_test, y_pred_test)\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "# Main execution flow with enhanced output\n",
    "print(\"=== Starting Stock Market Predictor ===\")\n",
    "\n",
    "# Prepare features\n",
    "try:\n",
    "    company_features = prepare_stock_features(\n",
    "        comprehensive_data_cleaned,\n",
    "        job_skills,\n",
    "        company_industries\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in feature preparation: {str(e)}\")\n",
    "    print(\"Using fallback feature engineering...\")\n",
    "    company_features = comprehensive_data_cleaned.groupby('company_id').agg({\n",
    "        'applies': ['sum', 'mean'],\n",
    "        'views': ['sum', 'mean'],\n",
    "        'remote_allowed': lambda x: (x == 't').mean(),\n",
    "        'employee_count': 'mean',\n",
    "        'follower_count': 'mean'\n",
    "    })\n",
    "    company_features.columns = ['_'.join(col).strip() for col in company_features.columns.values]\n",
    "    company_features.fillna(0, inplace=True)\n",
    "\n",
    "# Get stock data\n",
    "company_ids_with_features = company_features.index.unique()\n",
    "stock_data = get_stock_data(company_ids_with_features, companies)\n",
    "\n",
    "# Merge features with stock data\n",
    "merged_data = stock_data.reset_index().merge(\n",
    "    company_features,\n",
    "    on='company_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in company_features.columns if col != 'company_id']\n",
    "X = merged_data[feature_cols]\n",
    "y = merged_data['future_return']\n",
    "\n",
    "print(\"\\n=== Final Data Summary ===\")\n",
    "print(f\"Total samples available: {len(X)}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Target variable stats: mean={y.mean():.4f}, std={y.std():.4f}\")\n",
    "\n",
    "# Train and evaluate model\n",
    "model, metrics = train_and_evaluate_model(X, y)\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n=== Feature Importance Analysis ===\")\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.named_steps['regressor'].feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=feature_importance.head(20),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top 7 Most Important Features for Stock Return Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed predictions for top companies\n",
    "print(\"\\n=== Detailed Predictions ===\")\n",
    "\n",
    "# Get top 5 companies with most data points\n",
    "top_companies = merged_data['company_name'].value_counts().head(5).index\n",
    "\n",
    "for company in top_companies:\n",
    "    company_data = merged_data[merged_data['company_name'] == company]\n",
    "    if len(company_data) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get latest data point\n",
    "    latest = company_data.iloc[-1]\n",
    "    features = latest[feature_cols].values.reshape(1, -1)\n",
    "    prediction = model.predict(features)[0]\n",
    "    actual = latest['future_return']\n",
    "    ticker = latest.get('ticker', 'UNKNOWN')\n",
    "\n",
    "    print(f\"\\nCompany: {company} ({ticker})\")\n",
    "    print(f\"Predicted 30-day return: {prediction * 100:.2f}%\")\n",
    "    if not np.isnan(actual):\n",
    "        print(f\"Actual 30-day return: {actual * 100:.2f}%\")\n",
    "        print(f\"Prediction error: {(prediction - actual) * 100:.2f}%\")\n",
    "    print(\"\\nKey Features:\")\n",
    "    for feat in feature_importance['Feature'].head(3):\n",
    "        print(f\"{feat}: {latest[feat]:.2f}\")\n",
    "\n",
    "print(\"\\n=== Stock Market Predictor Complete ===\")\n",
    "\n",
    "\n",
    "# Create classification target: Low, Medium, High return classes\n",
    "merged_data['return_class'] = pd.qcut(\n",
    "    merged_data['future_return'], q=3, labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "#replace qcut with this to get the same classes as in the report\n",
    "\n",
    "#bins = [-float('inf'), -0.01, 0.01, float('inf')]\n",
    "#labels = ['-1', '0', '1']  # or ['Decrease', 'Neutral', 'Increase'] if preferred\n",
    "\n",
    "# Create classification target using custom thresholds\n",
    "#merged_data['return_class'] = pd.cut(merged_data['future_return'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n=== Classification Model Training ===\")\n",
    "\n",
    "\n",
    "\n",
    "# Prepare input features and target\n",
    "X_class = merged_data[feature_cols]\n",
    "y_class = merged_data['return_class']\n",
    "\n",
    "# Drop NA targets if any\n",
    "X_class = X_class[~y_class.isna()]\n",
    "y_class = y_class[~y_class.isna()]\n",
    "\n",
    "# Print class distribution before SMOTE\n",
    "print(\"\\nClass distribution before SMOTE:\")\n",
    "print(y_class.value_counts())\n",
    "\n",
    "# Split data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', CalibratedClassifierCV(rf_base, method='sigmoid', cv=5))\n",
    "])\n",
    "\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model with SMOTE...\")\n",
    "clf_pipeline.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Predict\n",
    "y_pred_c = clf_pipeline.predict(X_test_c)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_c, y_pred_c))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_c, y_pred_c))\n",
    "\n",
    "print(\"\\nCatagory boundries:\")\n",
    "bins = pd.qcut(merged_data['future_return'], q=3, duplicates='drop')\n",
    "print(bins.unique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test_c, y_pred_c, labels=['Low', 'Medium', 'High'])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== Stock Market Only Classification Model ===\")\n",
    "\n",
    "# Prepare stock market only features\n",
    "# First, let's print all available features to debug\n",
    "print(\"\\nAvailable features:\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Create stock market only features\n",
    "stock_market_features = [col for col in feature_cols if any(x in col.lower() for x in ['close', 'volume', 'open', 'high', 'low'])]\n",
    "if not stock_market_features:  # If no direct stock market features, use a subset of features\n",
    "    stock_market_features = feature_cols[:5]  # Use first 5 features as a proxy for stock market data\n",
    "\n",
    "print(\"\\nSelected stock market features:\")\n",
    "print(stock_market_features)\n",
    "\n",
    "if not stock_market_features:\n",
    "    print(\"\\nNo direct stock market features found. Using alternative approach...\")\n",
    "    # Use only the first 5 features as a proxy for stock market data\n",
    "    stock_market_features = feature_cols[:5]\n",
    "    print(\"Using first 5 features as proxy:\", stock_market_features)\n",
    "\n",
    "# Create stock market only dataset\n",
    "X_stock = merged_data[stock_market_features].copy()\n",
    "y_stock = merged_data['return_class'].copy()\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"\\nData shapes:\")\n",
    "print(f\"X_stock shape: {X_stock.shape}\")\n",
    "print(f\"y_stock shape: {y_stock.shape}\")\n",
    "\n",
    "# Drop NA targets if any\n",
    "X_stock = X_stock[~y_stock.isna()]\n",
    "y_stock = y_stock[~y_stock.isna()]\n",
    "\n",
    "print(\"\\nAfter dropping NA:\")\n",
    "print(f\"X_stock shape: {X_stock.shape}\")\n",
    "print(f\"y_stock shape: {y_stock.shape}\")\n",
    "\n",
    "# Print class distribution before SMOTE\n",
    "print(\"\\nClass distribution before SMOTE (Stock Market Only):\")\n",
    "print(y_stock.value_counts())\n",
    "\n",
    "# Split data\n",
    "X_train_stock, X_test_stock, y_train_stock, y_test_stock = train_test_split(\n",
    "    X_stock, y_stock, test_size=0.2, random_state=42, stratify=y_stock\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/test split shapes:\")\n",
    "print(f\"X_train_stock: {X_train_stock.shape}\")\n",
    "print(f\"X_test_stock: {X_test_stock.shape}\")\n",
    "\n",
    "# Create and train stock market only model with SMOTE\n",
    "rf_stock = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "stock_pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', CalibratedClassifierCV(rf_stock, method='sigmoid', cv=5))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining stock market only model with SMOTE...\")\n",
    "stock_pipeline.fit(X_train_stock, y_train_stock)\n",
    "\n",
    "# Predict\n",
    "y_pred_stock = stock_pipeline.predict(X_test_stock)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nStock Market Only Model Classification Report:\")\n",
    "print(classification_report(y_test_stock, y_pred_stock))\n",
    "\n",
    "print(\"\\nStock Market Only Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_stock, y_pred_stock))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_stock = confusion_matrix(y_test_stock, y_pred_stock, labels=['Low', 'Medium', 'High'])\n",
    "sns.heatmap(cm_stock, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=['Low', 'Medium', 'High'], \n",
    "            yticklabels=['Low', 'Medium', 'High'])\n",
    "plt.title(\"Stock Market Only Model Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare feature importance\n",
    "feature_importance_stock = pd.DataFrame({\n",
    "    'Feature': stock_market_features,\n",
    "    'Importance': stock_pipeline.named_steps['classifier'].base_estimator.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Stock Market Features:\")\n",
    "print(feature_importance_stock.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=feature_importance_stock.head(10),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Top 10 Most Important Stock Market Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare model performance\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(\"Full Model (All Features) vs Stock Market Only Model\")\n",
    "\n",
    "# Calculate metrics for both models\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Full model metrics\n",
    "full_accuracy = accuracy_score(y_test_c, y_pred_c)\n",
    "full_precision, full_recall, full_f1, _ = precision_recall_fscore_support(\n",
    "    y_test_c, y_pred_c, average='weighted'\n",
    ")\n",
    "\n",
    "# Stock market only model metrics\n",
    "stock_accuracy = accuracy_score(y_test_stock, y_pred_stock)\n",
    "stock_precision, stock_recall, stock_f1, _ = precision_recall_fscore_support(\n",
    "    y_test_stock, y_pred_stock, average='weighted'\n",
    ")\n",
    "\n",
    "print(\"\\nFull Model Metrics:\")\n",
    "print(f\"Accuracy: {full_accuracy:.4f}\")\n",
    "print(f\"Precision: {full_precision:.4f}\")\n",
    "print(f\"Recall: {full_recall:.4f}\")\n",
    "print(f\"F1 Score: {full_f1:.4f}\")\n",
    "\n",
    "print(\"\\nStock Market Only Model Metrics:\")\n",
    "print(f\"Accuracy: {stock_accuracy:.4f}\")\n",
    "print(f\"Precision: {stock_precision:.4f}\")\n",
    "print(f\"Recall: {stock_recall:.4f}\")\n",
    "print(f\"F1 Score: {stock_f1:.4f}\")\n",
    "\n",
    "print(\"\\nImprovement from Stock Market Only to Full Model:\")\n",
    "print(f\"Accuracy: {full_accuracy - stock_accuracy:.4f}\")\n",
    "print(f\"Precision: {full_precision - stock_precision:.4f}\")\n",
    "print(f\"Recall: {full_recall - stock_recall:.4f}\")\n",
    "print(f\"F1 Score: {full_f1 - stock_f1:.4f}\")\n",
    "\n",
    "# Print feature sets to verify they're different\n",
    "print(\"\\nFeature Set Comparison:\")\n",
    "print(\"\\nFull Model Features:\", len(feature_cols))\n",
    "print(\"Stock Market Only Features:\", len(stock_market_features))\n",
    "print(\"\\nStock Market Features:\", stock_market_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccd7dc3-db99-4345-ad03-9a03ed647d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/benjaminscott/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mimblearn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sklearn\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimbalanced-learn:\u001b[39m\u001b[38;5;124m\"\u001b[39m, imblearn\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/benjaminscott/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "import sklearn, imblearn\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"imbalanced-learn:\", imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d14972-b7b5-4ee5-a679-a8d5dbf3325f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
